# LLM_Blog_Writer 
  This project runs LLM model locally. So make sure you have good enought Ram and Cpu core to generate results quickly.

1.Download Llama model "llama-2-7b-chat.ggml" from hugging face ------> https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main

2.Make a root folder named as LLM or any name you want and create a sub folder named as "models" and save the Llama model there. 

3.Install the requirements 

4.Run the code as "streamlit run app.py"

5.Python version 3.9
